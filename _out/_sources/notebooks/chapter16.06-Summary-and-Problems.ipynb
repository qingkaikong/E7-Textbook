{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"images/book_cover.jpg\" width=\"120\">\n",
    "\n",
    "*This notebook contains an excerpt from the [Python Programming and Numerical Methods - A Guide for Engineers and Scientists](https://www.elsevier.com/books/python-programming-and-numerical-methods/kong/978-0-12-819549-9), the content is also available at [Berkeley Python Numerical Methods](https://pythonnumericalmethods.berkeley.edu/notebooks/Index.html).*\n",
    "\n",
    "*The copyright of the book belongs to Elsevier. We also have this interactive book online for a better learning experience. The code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work on [Elsevier](https://www.elsevier.com/books/python-programming-and-numerical-methods/kong/978-0-12-819549-9) or [Amazon](https://www.amazon.com/Python-Programming-Numerical-Methods-Scientists/dp/0128195495/ref=sr_1_1?dchild=1&keywords=Python+Programming+and+Numerical+Methods+-+A+Guide+for+Engineers+and+Scientists&qid=1604761352&sr=8-1)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [16.5 Least Square Regression for Nonlinear Functions](chapter16.05-Least-Square-Regression-for-Nonlinear-Functions.ipynb)  | [Contents](Index.ipynb) | [CHAPTER 17. Interpolation](chapter17.00-Interpolation.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "1. Mathematical models are used to understand, predict, and control engineering systems. These models consist of parameters that govern the way the model behaves.\n",
    "2. Given a set of experimental data, least squares regression is a method of finding a set of model parameters that fits the data well. That is, it minimizes the squared error between the model, or estimation function, and the data points.\n",
    "3. In linear least squares regression, the estimation function must be a linear combination of linearly independent basis functions.\n",
    "4. The set of parameters ${\\beta}$ can be determined by the least squares equation ${\\beta} = (A^T A)^{-1} A^T Y$, where the $j$-th column of $A$ is the $j$-th basis function evaluated at each $X$ data point.\n",
    "5. To estimate a nonlinear function, we can transform it into a linear estimation function or directly use least square regression to solve the non-linear function using *curve_fit* from scipy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Problems\n",
    "\n",
    "1. Repeat the multivariable calculus derivation of the least squares regression formula for an estimation function $\\hat{y}(x) = ax^2 + bx + c$ where $a, b$, and $c$ are the parameters.\n",
    "\n",
    "2. Write a function *my_ls_params(f, x, y)*, where *x* and *y* are arrays of the same size containing experimental data, and *f* is a list with each element a function object to a basis vector of the estimation function. The output argument, *beta*, should be an array of the parameters of the least squares regression for *x*, *y*, and *f*.\n",
    "\n",
    "3. Write a function *my_func_fit (x,y)*, where *x* and *y* are column vectors of the same size containing experimental data, and the function returns *alpha* and *beta* are the parameters of the estimation function $\\hat{y}(x) = {\\alpha} x^{{\\beta}}$.\n",
    "\n",
    "4. Given four data points $(x_i, y_i)$ and the parameters for a cubic polynomial $\\hat{y}(x) = ax^3 + bx^2 + cx + d$, what will be the total error associated with the estimation function $\\hat{y}(x)$? Where can we place another data point *(x,y)* such that no additional error is incurred for the estimation function?\n",
    "\n",
    "5. Write a function *my_lin_regression(f, x, y)*, where *f* is a list containing function objects to basis functions, and *x* and *y* are arrays containing noisy data. Assume that *x* and *y* are the same size. \n",
    "\n",
    " Let an estimation function for the data contained in *x* and *y* be defined as $\\hat{y}(x) = {\\beta}(1) \\cdot f_{1}(x) + {\\beta}(2) \\cdot f_{2}(x) + \\cdots + {\\beta}(n) \\cdot f_{n}(x)$, where *n* is the length of *f*. Your function should compute *beta* according to the least squares regression formula.\n",
    "\n",
    " Test Case: Note that your solution may vary by a little bit depending on the random numbers generated.\n",
    " \n",
    "```python\n",
    "x = np.linspace(0, 2*np.pi, 1000)\n",
    "y = 3*np.sin(x) - 2*np.cos(x) + np.random.random(len(x))\n",
    "f = [np.sin, np.cos]\n",
    "beta = my_lin_regression(f, x, y)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(x,y,'b.', label = 'data')\n",
    "plt.plot(x, beta[0]*f[0](x)+beta[1]*f[1](x)+beta[2], 'r', label='regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Least Square Regression Example')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<img src=\"./images/16.06.01-problem_5_example.jpg\" alt=\"problem 5 example\" title=\"Results from problem 5\" width=\"500\"/>\n",
    "\n",
    "6. Write a function *my_exp_regression (x,y)*, where *x* and *y* are arrays the same size.\n",
    "\n",
    " Let an estimation function for the data contained in *x* and *y* be defined as $\\hat{y}(x) = {\\alpha} e^{{\\beta} x}$. Your function should compute ${\\alpha}$ and ${\\beta}$ as the solution to the least squares regression formula.\n",
    "\n",
    " Test Cases: Note that your solution may vary from the test case slightly depending on the random numbers generated.\n",
    " \n",
    "```python\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y = 2*np.exp(-0.5*x) + 0.25*np.random.random(len(x))\n",
    "\n",
    "alpha, beta = my_exp_regression(x, y)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(x,y,'b.', label = 'data')\n",
    "plt.plot(x, alpha*np.exp(beta*x), 'r', label='regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Least Square Regression on Exponential Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"./images/16.06.02-problem_6_example.jpg\" alt=\"problem 5 example\" title=\"Results from problem 6\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [16.5 Least Square Regression for Nonlinear Functions](chapter16.05-Least-Square-Regression-for-Nonlinear-Functions.ipynb)  | [Contents](Index.ipynb) | [CHAPTER 17. Interpolation](chapter17.00-Interpolation.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
