{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"images/book_cover.jpg\" width=\"120\">\n",
    "\n",
    "*This notebook contains an excerpt from the [An Introduction To Python Programming And Numerical Methods For Scientists and Engineers](); the content is available [on GitHub]().*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book]()!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [8.1 Complexity and Big-O Notation](chapter08.01-Complexity-and-Big-O.ipynb) | [Contents](Index.ipynb) | [8.3 The Profiler](chapter08.03-The-Profiler.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why does complexity matter? Assume you have an algorithm that runs in exponential time, say $O(2^n)$, and let $N$ be the largest problem you can solve with this algorithm using the computational resources you have, denoted by $R$. $R$ could be the amount of time you are willing to wait for the function to finish, or $R$ could be the number of basic operations you watch the computer execute before you get sick of waiting. Using the same algorithm, how large of a problem can you solve given a new computer that is twice as fast?\n",
    "\n",
    "If we establish $R = 2^N$, using our old computer, with our new computer we have $2R$ computational resources; therefore, we want to find $N^{\\prime}$ such that $2R = 2^{N^{\\prime}}$. With some substitution, we can arrive at $2 \\times 2^N = 2^{N^{\\prime}}\\rightarrow 2^{N+1} = 2^{N^{\\prime}}\\rightarrow N' = N+1$. So with an exponential time algorithm, doubling your computational resources will allow you to solve a problem one unit larger than you could with your old computer. This is a very small difference. In fact as $N$ gets large, the relative improvement goes to 0.\n",
    "\n",
    "With a polynomial time algorithm, you can do much better. This time let's assume that $R = N^c$, where $c$ is some constant larger than one. Then $2R = {N^{\\prime}}^c$, which using similar substitutions as before gets you to $N^{\\prime} = 2^{1/c}N$. So with a polynomial time algorithm with power $c$, you can solve a problem $\\sqrt[c]{2}$ larger than you could with your old computer. When $c$ is small, say less than 5, this is a much bigger difference than with the exponential algorithm.\n",
    "\n",
    "Finally, let us consider a log time algorithm. Let $R = \\log{N}$. Then $2R = \\log{N^{\\prime}}$, and again with some substitution we obtain $N^{\\prime} = N^2$. So with the double resources, we can square the size of the problem we can solve!\n",
    "\n",
    "The moral of the story is that exponential time algorithms do not scale well. That is, as you increase the size of the input, you will soon find that the function takes longer (much longer) than you are willing to wait. For one final example, $my\\_fib\\_rec(100)$ would take on the order $2^{100}$ basic operations to perform. If your computer could do 100 trillion basic operations per second (far faster than the fastest computer on earth), it would take your computer about 400 million years to complete. However, $my\\_fib\\_iter(100)$ would take less than 1 nanosecond.\n",
    "\n",
    "There is both an exponential time algorithm (recursion) and a polynomial time algorithm (iteration) for computing Fibonacci numbers. Given a choice, we would clearly pick the polynomial time algorithm. However, there is a class of problems for which no one has ever discovered a polynomial time algorithm. In other words, there are only exponential time algorithms known for them. These problems are known as NP-Complete, and there is ongoing investigation as to whether polynomial time algorithms exist for these problems. Examples of NP-Complete problems include the Traveling Salesman, Set Cover, and Set Packing problems. Although theoretical in construction, solutions to these problems have numerous applications in logistics and operations research. In fact, some encryption algorithms that keep web and bank applications secure rely on the NP-Complete-ness of breaking them. A further discussion of NP-Complete problems and the theory of complexity is beyond the scope of this book but these problems are very interesting and important to many engineering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [8.1 Complexity and Big-O Notation](chapter08.01-Complexity-and-Big-O.ipynb) | [Contents](Index.ipynb) | [8.3 The Profiler](chapter08.03-The-Profiler.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
