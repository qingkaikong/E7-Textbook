{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"images/book_cover.jpg\" width=\"120\">\n",
    "\n",
    "*This notebook contains an excerpt from the [An Introduction To Python Programming And Numerical Methods For Scientists and Engineers](); the content is available [on GitHub]().*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book]()!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [CHAPTER 8. Complexity](chapter08.00-Complexity.ipynb) | [Contents](Index.ipynb) | [8.2 Complexity Matters](chapter08.02-Complexity-Matters.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity and Big-O Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **complexity** of a function is the relationship between the size of the input and the difficulty of running the function to completion. The size of the input is usually denoted by *n*. In computer science, this is usually taken to be the number of bits required to describe the problem. However, *n* usually describes something more tangible, such as the length of an array. The difficulty of a problem can be measured in several ways. It can be measured by the number of **bit operations** needed for the function to finish, which means the number of times a 1 must be turned to a 0 and vice versa, as well as a few other simple things that computers can do. It is usually more suitable to describe the difficulty of the problem in terms of **basic operations**: additions, subtractions, multiplications, divisions, assignments, and function calls. Although each basic operation takes different amounts of time, the number of basic operations needed to complete a function is sufficiently related to the running time to be useful, and it is much easier to count.\n",
    "\n",
    "**TRY IT!** Count the number of basic operations, in terms of *n*, required for the following function to terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(n):\n",
    "    out = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            out += i*j\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the number of operations:\n",
    "\n",
    "additions: $n^2$, subtractions: 0, multiplications: $n^2$, divisions: 0, assignments: $2n^2 +1$, function calls: 0, total: $4n^2+1$.\n",
    "\n",
    "The number of assignments is $2n^2 + n + 1$ because the line _out += i*j_ is evaluated $n^2$ times, *j* is assigned $n^2$, *i* is assigned *n* times, and the line *out = 0* is assigned once. So, the complexity of the function *f* can be described as $4n^2 + n + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common notation for complexity is called **Big-O notation**. Big-O notation establishes the relationship in the growth of the number of basic operations with respect to the size of the input as the input size becomes very large. As _n_ gets large, the highest power dominates; therefore, only the highest power term is included in Big-O notation. Additionally, coefficients are not required to characterize growth, and so coefficients are also dropped. In the previous example, we counted $4n^2 + n + 1$ basic operations to complete the function. In Big-O notation we would say that the function is $O(n^2)$ (pronounced \"O of n-squared\"). We say that any algorithm with complexity $O(nc)$ where _c_ is some constant with respect to _n_ is **polynomial time**.\n",
    "\n",
    "**TRY IT!** Determine the complexity of the iterative Fibonacci function in Big-O notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fib_iter(n):\n",
    "    \n",
    "    out = [1, 1]\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        out.append(out[i - 1] + out[i - 2])\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the only lines of code that take more time as _n_ grows are those in the for-loop, we can restrict our attention to the for-loop and the code block within it. The code within the for-loop does not grow with respect to _n_ (i.e., it is constant). Therefore, the number of basic operations is $Cn$ where _C_ is some constant representing the number of basic operations that occur in the for-loop, and these _C_ operations run _n_ times. This gives a complexity of $O(n)$ for $my\\_fib\\_iter$.\n",
    "\n",
    "Assessing the exact complexity of a function can be difficult. In these cases, it might be sufficient to give an upper bound or even an approximation of the complexity.\n",
    "\n",
    "**TRY IT!** Give an upper bound on the complexity of the recursive implementation of Fibonacci. Do you think it is a good approximation of the upper bound? Do you think that recursive Fibonacci could possibly be polynomial time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fib_rec(n):\n",
    "    \n",
    "    if n < 2:\n",
    "        out = 1\n",
    "    else:\n",
    "        out = my_fib_rec(n-1) + my_fib_rec(n-2)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As *n* gets large, we can say that the vast majority of function calls make two other function calls: one addition and one assignment to the output. The addition and assignment do not grow with *n* per function call, so we can ignore them in Big-O notation. However, the number of function calls grows approximately by $2^n$ , and so the complexity of $my\\_fib\\_rec$ is upper bound by $O(2^n)$.\n",
    "\n",
    "There is on-going debate whether or not $O(2^n)$ is a good approximation for the Fibonacci function.\n",
    "\n",
    "Since the number of recursive calls grows exponentially with *n*, there is no way the recursive fibonacci function could be polynomial. That is, for any *c*, there is an *n* such that $my\\_fib\\_rec$ takes more than $O(n^c)$ basic operations to complete. Any function that is $O(c^n)$ for some constant c is said to be **exponential time**.\n",
    "\n",
    "**TRY IT!** What is the complexity of the following function in Big-O notation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_divide_by_two(n):\n",
    "    \n",
    "    out = 0\n",
    "    while n > 1:\n",
    "        n /= 2\n",
    "        out += 1\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, only the while-loop runs longer for larger *n* so we can restrict our attention there. Within the while-loop, there are two assignments: one division and one addition, which are both constant\n",
    "time with respect to *n*. So the complexity depends only on how many times the while-loop runs.\n",
    "\n",
    "The while-loop cuts *n* in half in every iteration until *n* is less than 1. So the number of iterations, *I*, is the solution to the equation $\\frac{n}{2^I} = 1$. With some manipulation, this solves to $I = \\log n$, so the complexity of $my\\_divide\\_by\\_two$ is $O(log n)$. It does not matter what the base of the log is because, recalling log rules, all logs are a scalar multiple of each other. Any function with complexity $O(\\log n)$. is said to be __log time__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [CHAPTER 8. Complexity](chapter08.00-Complexity.ipynb) | [Contents](Index.ipynb) | [8.2 Complexity Matters](chapter08.02-Complexity-Matters.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
