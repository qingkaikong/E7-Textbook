{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"images/book_cover.jpg\" width=\"120\">\n",
    "\n",
    "*This notebook contains an excerpt from the [An Introduction To Python Programming And Numerical Methods For Scientists and Engineers](); the content is available [on GitHub]().*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book]()!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [11.3 Pickle Files](chapter11.03-Pickle-Files.ipynb) | [Contents](Index.ipynb) | [11.5 Summary and Problems](chapter11.05-Summary-and-Problems.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scientific computing, sometimes, we need to store large amounts of data with quick access, the file formats we introduced before are not going to cut it. You will soon find there are many cases, HDF5 (Hierarchical Data Format) is the solution. It is a powerful binary data format with no limit on the file size. It provides parallel IO (input/output), and carries out a bunch of low level optimizations under the hood to make the queries faster and storage requirements smaller. \n",
    "\n",
    "An HDF5 file saves two types of objects: *datasets*, which are array-like collections of data (like NumPy arrays), and *groups*, which are folder-like containers that hold datasets and other groups. There are also attributes that could associate with the datasets and groups to describe some properties. The so called *hierarchical* in HDF5 refers to the fact that the data could be saved like a file system, with folder-like structures, such as folder, subfolder (in HDF5, it is called group, subgroup). Groups operate like dictionaries with the *keys* and *values*, with the *keys* are names of the groups, and the *values* are the subgroups or datasets.  \n",
    "\n",
    "In order to use read/write HDF5 in Python, there are some packages or wrappers to serve the purposes. The most common two packages are [PyTables](https://www.pytables.org) and [h5py](https://www.h5py.org). We will only introduce the h5py here. You can install h5py use conda (hope you still remember how to do that, if you forget, please go back to Chapter 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we installed h5py, you can follow the quick start guide in [h5py documentation](http://docs.h5py.org/en/latest/quick.html) to get a quick start. But here, let's use one example to show how do we create, and read a HDF5 file. Let's import the NumPy and h5py first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Suppose we deployed some instruments to monitor the accelerations and GPS location in Bay Area, CA. We deployed two accelerometers at Berkeley and Oakland as well as one GPS station at San Fransisco. And they record data at different sampling rates, with the accelerometer at Berkeley sample the data every 0.04 s, and 0.01 s for the sensor at Oakland. The GPS samples the location every 60 seconds in San Fransisco. Now we want to store the two types of data into a HDF5 as well as some attributes indicate where the data is recorded, start time of the recording, station name and the sampling interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data for recording\n",
    "acc_1 = np.random.random(1000)\n",
    "station_number_1 = '1'\n",
    "# unix timestamp\n",
    "start_time_1 = 1542000276\n",
    "# time interval for recording\n",
    "dt_1 = 0.04\n",
    "location_1 = 'Berkeley'\n",
    "\n",
    "acc_2 = np.random.random(500)\n",
    "station_number_2 = '2'\n",
    "start_time_2 = 1542000576\n",
    "dt_2 = 0.01\n",
    "location_2 = 'Oakland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('station.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf['/acc/1/data'] = acc_1\n",
    "hf['/acc/1/data'].attrs['dt'] = dt_1\n",
    "hf['/acc/1/data'].attrs['start_time'] = start_time_1\n",
    "hf['/acc/1/data'].attrs['location'] = location_1\n",
    "\n",
    "hf['/acc/2/data'] = acc_2\n",
    "hf['/acc/2/data'].attrs['dt'] = dt_2\n",
    "hf['/acc/2/data'].attrs['start_time'] = start_time_2\n",
    "hf['/acc/2/data'].attrs['location'] = location_2\n",
    "\n",
    "hf['/gps/1/data'] = np.random.random(100)\n",
    "hf['/gps/1/data'].attrs['dt'] = 60\n",
    "hf['/gps/1/data'].attrs['start_time'] = 1542000000\n",
    "hf['/gps/1/data'].attrs['location'] = 'San Francisco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows the core concepts in HDF5: the groups, datasets, attributes. We first create an HDF5 object for writing - station.hdf5. Then we start to store the data to different groups. We can see we have two top level groups, i.e. acc and gps, both of them contains subgroups 1 or 2 indicate the station names. Each station will contain the next level subgroup, data, that is used to store the array data we created. We could then add attributes to the groups or the data. Here we only added the *dt*, *start_time*, and *location* as the attributes to the datasets we store here. You can see that it is quite similar to folder-like structure, with data *acc_1* saved at */acc/1/data*. Lastly, we close the file object. \n",
    "\n",
    "Now we can see that saving data in HDF5 is easy, and we could use function *create_dataset* and *create_group* as shown in the [quick start](http://docs.h5py.org/en/latest/quick.html). But I am more prefer to use the above approach to create multiple intermediate groups implicitly as getting access to a folder structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the HDF5 files\n",
    "\n",
    "Now suppose you send the *station.hdf5* to a colleague, who wants to get access to the data. Here is how he/she will do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_in = h5py.File('station.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc', 'gps']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_in.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hf_in['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(acc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = hf_in['acc/1/data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41820889, 0.89832446, 0.40229251, 0.41287538, 0.16173359,\n",
       "       0.75855904, 0.89288185, 0.82944522, 0.84228139, 0.50365515])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.value[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dt', 'start_time', 'location']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_1.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.attrs['dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Berkeley'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.attrs['location']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see reading a HDF5 is also easy with *h5py*. After we read in the HDF5 to *hf_in*, we could see what groups are in the HDF5 using the *keys* function. Then we could get access to the group members and see what contains in the subgroups as the *hf_in['acc']*, or directly specify the path to the datasets as *hf_in['acc/1/data']* and get the array data. Of course, the attributes that associated with the data could also be accessed as a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [11.3 Pickle Files](chapter11.03-Pickle-Files.ipynb) | [Contents](Index.ipynb) | [11.5 Summary and Problems](chapter11.05-Summary-and-Problems.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
